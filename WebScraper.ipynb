{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a99cd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from newspaper import Article\n",
    "from crawl4ai import *\n",
    "import scrapy\n",
    "\n",
    "class ArticleSpider(scrapy.Spider):\n",
    "    name = \"article\"\n",
    "    start_urls = []  # wird dynamisch übergeben\n",
    "\n",
    "    def __init__(self, url=None, *args, **kwargs):\n",
    "        super(ArticleSpider, self).__init__(*args, **kwargs)\n",
    "        if url:\n",
    "            self.start_urls = [url]\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Alle relevanten Textblöcke sammeln\n",
    "        text_blocks = response.css(\"p::text, div::text, span::text, h1::text, h2::text, h3::text\").getall()\n",
    "        \n",
    "        # Nur sinnvolle Texte nehmen\n",
    "        clean_text = ' '.join(\n",
    "            [t.strip() for t in text_blocks if t and len(t.strip()) > 30]  # Filter gegen Navigation/Trash\n",
    "        )\n",
    "\n",
    "        yield {\"text\": clean_text}\n",
    "\n",
    "\n",
    "def search_news(stock_symbol, NEWS_API_KEY, max_results=5):\n",
    "    url = \"https://newsapi.org/v2/everything\"\n",
    "    params = {\n",
    "        \"q\": stock_symbol,\n",
    "        \"sortBy\": \"publishedAt\",\n",
    "        \"language\": \"en\",\n",
    "        \"pageSize\": max_results,\n",
    "        \"apiKey\": NEWS_API_KEY,\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    print(response)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(\"Fehler beim Abrufen der News:\", response.json())\n",
    "        return []\n",
    "\n",
    "    data = response.json()\n",
    "    links = [article[\"url\"] for article in data.get(\"articles\", [])]\n",
    "    return links\n",
    "\n",
    "\n",
    "import requests\n",
    "from newspaper import Article\n",
    "\n",
    "def extract_article_text(url):\n",
    "    \"\"\"Extracts the full article text with custom headers.\"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "                          \"(KHTML, like Gecko) Chrome/114.0 Safari/537.36\"\n",
    "        }\n",
    "        article = Article(url)\n",
    "        article.download(input_html=requests.get(url, headers=headers).text)\n",
    "        article.parse()\n",
    "        return article.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting article from {url}: {e}\")\n",
    "        return \"\"\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
