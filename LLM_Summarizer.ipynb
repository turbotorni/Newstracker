{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ecdbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import nltk\n",
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "# Download required NLTK data if not already present\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "class EnhancedSummarizer:\n",
    "    def __init__(self, model_name: str):\n",
    "        \"\"\"\n",
    "        Initialize the enhanced summarizer with CPU-optimized settings.\n",
    "        :param model_name: The name of the summarization model to use.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.device = -1  # Force CPU usage for 16GB RAM constraint\n",
    "        # The prompt is added here to guide the summarization model\n",
    "        self.prompt = \"Please provide a concise and brief summary of the following text:\"\n",
    "        \n",
    "        # Initialize the summarization pipeline\n",
    "        self.summarizer = pipeline(\n",
    "            \"summarization\",\n",
    "            model=model_name,\n",
    "            device=self.device,\n",
    "            torch_dtype=torch.float32  # Use float32 for better CPU performance\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Cleaning the input text.\n",
    "        :param text: The raw input text.\n",
    "        :return: The cleaned text.\n",
    "        \"\"\"\n",
    "        # Remove extra whitespace and normalize\n",
    "        text = re.sub(r'\\s+', ' ', text.strip())\n",
    "        \n",
    "        # Remove URLs and email addresses\n",
    "        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # Remove excessive punctuation\n",
    "        text = re.sub(r'[.]{2,}', '.', text)\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def extract_key_information(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        A simple method to extract key information like numbers and percentages.\n",
    "        (This is a placeholder implementation)\n",
    "        :param text: The cleaned input text.\n",
    "        :return: A dictionary containing extracted key information.\n",
    "        \"\"\"\n",
    "        # Regex to find potential stock prices (e.g., $100.00)\n",
    "        prices = re.findall(r'\\$\\d+\\.?\\d*', text)\n",
    "        \n",
    "        # Regex to find percentages (e.g., 50%)\n",
    "        percentages = re.findall(r'\\d+\\.?\\d*%', text)\n",
    "        \n",
    "        return {\n",
    "            'prices': prices,\n",
    "            'percentages': percentages\n",
    "        }\n",
    "    \n",
    "    def smart_chunk_text(self, text: str, max_tokens: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Splits the text into chunks, respecting sentence boundaries.\n",
    "        (This is a placeholder implementation)\n",
    "        :param text: The input text.\n",
    "        :param max_tokens: The maximum number of tokens per chunk.\n",
    "        :return: A list of text chunks.\n",
    "        \"\"\"\n",
    "        sentences = sent_tokenize(text)\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        for sentence in sentences:\n",
    "            # Check token count for the new sentence\n",
    "            sentence_tokens = len(self.tokenizer.encode(sentence))\n",
    "            current_chunk_tokens = len(self.tokenizer.encode(current_chunk))\n",
    "            \n",
    "            # If the current chunk plus the new sentence exceeds the limit,\n",
    "            # start a new chunk\n",
    "            if current_chunk_tokens + sentence_tokens > max_tokens and current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence + \" \"\n",
    "            else:\n",
    "                current_chunk += sentence + \" \"\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            \n",
    "        return chunks\n",
    "\n",
    "    def generate_multi_stage_summary(self, text: str, max_tokens: int) -> str:\n",
    "        \"\"\"\n",
    "        Generate summary using multiple stages for better quality.\n",
    "        :param text: The input text to be summarized.\n",
    "        :param max_tokens: The maximum number of tokens for a single model call.\n",
    "        :return: The final summarized text.\n",
    "        \"\"\"\n",
    "        # Stage 1: Preprocess\n",
    "        clean_text = self.preprocess_text(text)\n",
    "        \n",
    "        # Stage 2: Extract key info (e.g., numbers, percentages)\n",
    "        key_info = self.extract_key_information(clean_text)\n",
    "        \n",
    "        # Stage 3: Smart chunking\n",
    "        chunks = self.smart_chunk_text(clean_text, max_tokens)\n",
    "        \n",
    "        if not chunks:\n",
    "            return \"Unable to process the provided text.\"\n",
    "        \n",
    "        # Stage 4: Summarize chunks\n",
    "        chunk_summaries = []\n",
    "        for chunk in chunks:\n",
    "            try:\n",
    "                # Prepend the prompt to each chunk for better LLM performance\n",
    "                chunk_with_prompt = f\"{self.prompt} {chunk}\"\n",
    "                \n",
    "                # Adjust summary length based on chunk size\n",
    "                chunk_tokens = len(self.tokenizer.encode(chunk_with_prompt))\n",
    "                max_len = min(150, max(50, chunk_tokens // 4))\n",
    "                min_len = min(40, max_len // 3)\n",
    "                \n",
    "                summary = self.summarizer(\n",
    "                    chunk_with_prompt,\n",
    "                    max_length=max_len,\n",
    "                    min_length=min_len,\n",
    "                    do_sample=False,\n",
    "                    length_penalty=1.0,\n",
    "                    repetition_penalty=1.1\n",
    "                )\n",
    "                chunk_summaries.append(summary[0]['summary_text'])\n",
    "            except Exception as e:\n",
    "                print(f\"Error summarizing chunk: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Stage 5: Combine and final summarization if multiple chunks\n",
    "        if len(chunk_summaries) > 1:\n",
    "            combined_text = \" \".join(chunk_summaries)\n",
    "            try:\n",
    "                # Prepend prompt for the final summary\n",
    "                combined_with_prompt = f\"{self.prompt} {combined_text}\"\n",
    "                final_summary = self.summarizer(\n",
    "                    combined_with_prompt,\n",
    "                    max_length=1024,\n",
    "                    min_length=80,\n",
    "                    do_sample=False,\n",
    "                    length_penalty=1.0\n",
    "                )\n",
    "                result = final_summary[0]['summary_text']\n",
    "            except:\n",
    "                result = \" \".join(chunk_summaries[:2])  # Fallback\n",
    "        else:\n",
    "            result = chunk_summaries[0] if chunk_summaries else \"No summary generated.\"\n",
    "        \n",
    "        # Stage 6: Post-process and enhance\n",
    "        result = self.post_process_summary(result, key_info)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def post_process_summary(self, summary: str, key_info: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Post-process the summary to add structure and key information.\n",
    "        (This is a placeholder implementation)\n",
    "        :param summary: The generated summary text.\n",
    "        :param key_info: A dictionary of key information to potentially add back.\n",
    "        :return: The enhanced summary.\n",
    "        \"\"\"\n",
    "        # A simple check to ensure key numbers are preserved\n",
    "        if 'prices' in key_info and key_info['prices']:\n",
    "            if not any(price in summary for price in key_info['prices'][:2]):\n",
    "                summary = f\"NVIDIA stock trades around {key_info['prices'][0]}. \" + summary\n",
    "        \n",
    "        if 'percentages' in key_info and key_info['percentages']:\n",
    "            # Make sure at least one key percentage is mentioned\n",
    "            key_percentages = [p for p in key_info['percentages'] if float(p.replace('%', '')) > 10]\n",
    "            if key_percentages and not any(pct in summary for pct in key_percentages[:2]):\n",
    "                summary += f\" Notable growth includes {key_percentages[0]} performance metrics.\"\n",
    "        \n",
    "        return summary\n",
    "\n",
    "def get_summary(text: str, max_tokens: int, model_name: str = \"facebook/bart-large-cnn\") -> str:\n",
    "    \"\"\"\n",
    "    Main function to summarize text with enhanced processing.\n",
    "    :param text: The text to summarize.\n",
    "    :param max_tokens: The maximum number of tokens per model call.\n",
    "    :param model_name: The name of the Hugging Face model to use.\n",
    "    :return: The generated summary.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # For CPU-only systems with limited RAM, consider these alternatives:\n",
    "        # - \"sshleifer/distilbart-cnn-12-6\" (smaller, faster)\n",
    "        # - \"google/pegasus-xsum\" (good for news)\n",
    "        # - \"philschmid/bart-large-cnn-samsum\" (conversational)\n",
    "        \n",
    "        summarizer = EnhancedSummarizer(model_name)\n",
    "        summary = summarizer.generate_multi_stage_summary(text, max_tokens)\n",
    "        return summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in summarization: {e}\")\n",
    "        return \"Error: Unable to generate summary. Try with a smaller text or a different model.\"\n",
    "\n",
    "# Example usage (uncomment to test)\n",
    "# sample_text = \"NVIDIA's recent financial report shows a 50% increase in Q3 revenue. The company's stock is currently trading at $150.00 per share, a significant rise from last month's $100.00. Analysts attribute this growth to strong demand for their new AI chips. The company is also investing heavily in new research and development projects to stay ahead of competitors.\"\n",
    "# print(get_summary(sample_text, max_tokens=1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71d9962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nvidia_text = \"\"\"Your paste.txt content here...\"\"\"\n",
    "    \n",
    "# # different modellist\n",
    "# models_to_try = [\n",
    "#     \"sshleifer/distilbart-cnn-12-6\",\n",
    "#     \"facebook/bart-large-cnn\",        \n",
    "#     \"google/pegasus-xsum\"\n",
    "# ]\n",
    "    \n",
    "# for model in models_to_try:\n",
    "#     try:\n",
    "#         print(f\"\\n--- Using model: {model} ---\")\n",
    "#         summary = summarize_nvidia_news(nvidia_text, model)\n",
    "#         print(summary)\n",
    "#         break\n",
    "#     except Exception as e:\n",
    "#         print(f\"Model {model} failed: {e}\")\n",
    "#         continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
